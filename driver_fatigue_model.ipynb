{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avidnerd/DriverFatigueDetector/blob/main/driver_fatigue_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaxP4yOAWmx_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.load('/content/drive/MyDrive/subhi/saved_data/X1.npy')\n",
        "y = np.load('/content/drive/MyDrive/subhi/saved_data/y1.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8uXHdNGP2jy"
      },
      "outputs": [],
      "source": [
        "!pip install EMD-signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuG8UuOngIMZ"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==0.9.1\n",
        "!pip install min2net==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMcuM2ZeOei_"
      },
      "outputs": [],
      "source": [
        "from PyEMD import EMD\n",
        "import numpy as np\n",
        "\n",
        "emd = EMD()\n",
        "data = []\n",
        "\n",
        "#creating the IMFs\n",
        "time = np.array([[i for i in range(2501)] for i in range(6)])\n",
        "for i in range(len(drowsy)):\n",
        "    print(drowsy[i].shape)\n",
        "    print(T.shape)\n",
        "    imf = emd(drowsy[i], time)\n",
        "    data.append(imf)\n",
        "\n",
        "data = np.array(data)\n",
        "\n",
        "num_samples = len(data[0][0])\n",
        "num_imfs = 0\n",
        "num_frames = data.shape[0]\n",
        "num_channels = data.shape[1]\n",
        "\n",
        "#finding the maximum number of IMFs\n",
        "for i in range(num_frames):\n",
        "    for j in range(num_channels):\n",
        "        item_imf = np.array(data[i, j])\n",
        "        if item_imf.shape[1] > num_imfs:\n",
        "            num_imfs = item_imf.shape[1]\n",
        "\n",
        "data_trans = np.zeros([num_samples, num_imfs, num_frames, num_channels])\n",
        "for i in range(num_frames):\n",
        "    for j in range(num_channels):\n",
        "        item_imf = np.array(data[i, j])\n",
        "        data_trans[:, :item_imf.shape[1], i, j] = item_imf\n",
        "\n",
        "data = data_trans\n",
        "\n",
        "set_real_FRMs_idx = [0, 1, 2, 3, 4]\n",
        "num_arti_FRMs = 500\n",
        "set_real_IMFs = data\n",
        "set_real_IMFs_idx = [0, 1, 2]\n",
        "\n",
        "def create_artifical_frame(set_real_FRMs_idx: list, num_arti_FRMs: int, set_real_IMFs: np.array, set_real_IMFs_idx: list):\n",
        "    num_samples, num_channels = set_real_IMFs.shape[0], set_real_IMFs.shape[-1]\n",
        "\n",
        "    arti_FRMs = np.zeros([num_arti_FRMs, num_samples, num_channels])\n",
        "    for idx_FRM in range(num_arti_FRMs):\n",
        "        max_arti_IMFs = len(set_real_IMFs_idx)\n",
        "        item_FRM_IMFs = np.zeros([num_samples, max_arti_IMFs, num_channels])\n",
        "\n",
        "        seq_FRMs_idx = []\n",
        "        while(len(seq_FRMs_idx) < max_arti_IMFs):\n",
        "            seq_FRMs_idx.extend(np.random.permutation(len(set_real_FRMs_idx)))\n",
        "\n",
        "        seq_FRMs_idx = [int(item) for item in seq_FRMs_idx]\n",
        "\n",
        "        seq_FRMs_idx = seq_FRMs_idx[:max_arti_IMFs]\n",
        "\n",
        "        for idx_IMF in range(max_arti_IMFs):\n",
        "            for idx_channel in range(num_channels):\n",
        "                idx_IMF_real = set_real_IMFs_idx[idx_IMF]\n",
        "                idx_FRM_real = set_real_FRMs_idx[seq_FRMs_idx[idx_IMF]]\n",
        "                selected_IMF = set_real_IMFs[:,\n",
        "                                             idx_IMF_real, idx_FRM_real, idx_channel]\n",
        "                item_FRM_IMFs[:, idx_IMF, idx_channel] = selected_IMF\n",
        "\n",
        "        arti_FRMs[idx_FRM, :, :] = np.sum(item_FRM_IMFs, 1)\n",
        "    return arti_FRMs\n",
        "\n",
        "arti_frames = create_artifical_frame(set_real_FRMs_idx,\n",
        "                                    num_arti_FRMs,\n",
        "                                    set_real_IMFs,\n",
        "                                    set_real_IMFs_idx)\n",
        "\n",
        "drowsy = np.concatenate((drowsy, arti_frames), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hjj9MEnNfEf"
      },
      "outputs": [],
      "source": [
        "X = np.concatenate((alert,drowsy,transitioning),axis=0)\n",
        "y = np.array([0]*len(alert)+[1]*len(drowsy)+[2]*len(transitioning))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fNRwKDVSNhL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_ratio = 0.80\n",
        "validation_ratio = 0.10\n",
        "test_ratio = 0.10\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_val = to_categorical(y_val, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO-TxTYqaKHv"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del X\n",
        "del y\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nPo7JhkjlEq"
      },
      "outputs": [],
      "source": [
        "print(X_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7TMHBIVdvi1"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "X_train = X_train.reshape((19870, 2501, 5))\n",
        "X_test = X_test.reshape((2484, 2501, 5))\n",
        "X_val = X_val.reshape((2484, 2501, 5))\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G5EopqWMpJF"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKE9nhcD9maZ"
      },
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8xNN9rDGDmI"
      },
      "outputs": [],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-pcfWoH99BJj"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "finX_train = []\n",
        "# Load the EEG data\n",
        "for i in range(len(X_train)):\n",
        "    data = X_train[i]\n",
        "    print(y_train[i])\n",
        "    # Extract the EEG data from the data\n",
        "    channel_index = 2\n",
        "    data = data[:, channel_index]\n",
        "\n",
        "    spg, freq, t, im = plt.specgram(data, NFFT=256, cmap='jet',Fs=256, noverlap=128)\n",
        "\n",
        "    normalized_spg = (spg - np.min(spg)) / (np.max(spg) - np.min(spg))\n",
        "\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    rgb_image = cmap(normalized_spg)\n",
        "\n",
        "    rgb_image = rgb_image[:,:,:3]\n",
        "    plt.xlabel('Time (samples)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.title('Time-Frequency Spectrogram of EEG Data')\n",
        "    rgb_images_array = np.array(rgb_image)\n",
        "    print(rgb_images_array.shape)\n",
        "    print(rgb_images_array)\n",
        "    finX_train.append(rgb_images_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJMgpmr3dE4k"
      },
      "outputs": [],
      "source": [
        "finX_train = np.array(finX_train)\n",
        "# finX_test = np.array(finX_test)\n",
        "# finX_val = np.array(finX_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vV_NiIO-23r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/finX_train_alpha.npy', finX_train)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/finy_train_alpha.npy', y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gnA3zkzTdrm3"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "finX_test = []\n",
        "# Load the EEG data\n",
        "for i in range(len(X_test)):\n",
        "    data = X_test[i]\n",
        "    # Extract the EEG data from the data\n",
        "    channel_index = 2\n",
        "    data = data[:, channel_index]\n",
        "\n",
        "    spg, freq, t, im = plt.specgram(data, NFFT=256, cmap='jet',Fs=256, noverlap=128)\n",
        "\n",
        "    normalized_spg = (spg - np.min(spg)) / (np.max(spg) - np.min(spg))\n",
        "\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    rgb_image = cmap(normalized_spg)\n",
        "\n",
        "    rgb_image = rgb_image[:,:,:3]\n",
        "\n",
        "    rgb_images_array = np.array(rgb_image)\n",
        "    print(rgb_images_array.shape)\n",
        "    print(rgb_images_array)\n",
        "    finX_test.append(rgb_images_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zuyYCxAqfDHd"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "finX_val = []\n",
        "# Load the EEG data\n",
        "for i in range(len(X_val)):\n",
        "    data = X_val[i]\n",
        "    # Extract the EEG data from the data\n",
        "    channel_index = 2\n",
        "    data = data[:, channel_index]\n",
        "\n",
        "    spg, freq, t, im = plt.specgram(data, NFFT=256, cmap='jet',Fs=256, noverlap=128)\n",
        "\n",
        "    normalized_spg = (spg - np.min(spg)) / (np.max(spg) - np.min(spg))\n",
        "\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    rgb_image = cmap(normalized_spg)\n",
        "\n",
        "    rgb_image = rgb_image[:,:,:3]\n",
        "\n",
        "    rgb_images_array = np.array(rgb_image)\n",
        "    print(rgb_images_array.shape)\n",
        "    print(rgb_images_array)\n",
        "    finX_val.append(rgb_images_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LHNrmRmIgFZr"
      },
      "outputs": [],
      "source": [
        "finX_train = np.array(finX_train)\n",
        "finX_test = np.array(finX_test)\n",
        "finX_val = np.array(finX_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1WrzsZEUKs-_"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/subhi/saved_data/finX_train_alpha.npy', finX_train)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/finX_test_alpha.npy', finX_test)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/finX_val_alpha.npy', finX_val)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/y_train_alpha.npy', y_train)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/y_test_alpha.npy', y_test)\n",
        "np.save('/content/drive/MyDrive/subhi/saved_data/y_val_alpha.npy', y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8W5qK199WiI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "finX_train = np.load('/content/drive/MyDrive/subhi/saved_data/finX_train.npy')\n",
        "finX_test = np.load('/content/drive/MyDrive/subhi/saved_data/finX_test.npy')\n",
        "finX_val = np.load('/content/drive/MyDrive/subhi/saved_data/finX_val.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBFcYmqPMwBi"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Reshape, Flatten, Dense, BatchNormalization, LSTM\n",
        "from keras import metrics\n",
        "\n",
        "def create_model(trial):\n",
        "    # Define the CNN part of the model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=trial.suggest_int('filters_1', 16, 64), kernel_size=trial.suggest_int('kernel_size_1', 2, 5), activation='relu', input_shape=(129, 18, 3)))\n",
        "    model.add(Conv2D(filters=trial.suggest_int('filters_2', 16, 64), kernel_size=trial.suggest_int('kernel_size_2', 2, 5), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(trial.suggest_float('dropout_1', 0.1, 0.5)))\n",
        "    model.add(MaxPooling2D(pool_size=trial.suggest_int('pool_size_1', 2, 4)))\n",
        "\n",
        "    model.add(Conv2D(filters=trial.suggest_int('filters_5', 16, 64), kernel_size=trial.suggest_int('kernel_size_5', 1, 2), activation='relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Reshape((1, -1)))\n",
        "    model.add(LSTM(trial.suggest_int('lstm_units', 30, 128), return_sequences=False))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', metrics.AUC(name='my_auc')])\n",
        "    return model\n",
        "\n",
        "def objective(trial):\n",
        "    model = create_model(trial)\n",
        "\n",
        "    history = model.fit(finX_train, y_train, validation_data=(finX_val, y_val), epochs=15, batch_size=32, verbose=1)\n",
        "\n",
        "    return history.history['val_accuracy'][-1]\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial_ = study.best_trial\n",
        "\n",
        "print(\" Value: {}\".format(trial_.value))\n",
        "\n",
        "print(\" Params: \")\n",
        "for key, value in trial_.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghAbzLaT935p"
      },
      "outputs": [],
      "source": [
        "y_train = np.load('/content/drive/MyDrive/subhi/saved_data/y_train.npy')\n",
        "y_test = np.load('/content/drive/MyDrive/subhi/saved_data/y_test.npy')\n",
        "y_val = np.load('/content/drive/MyDrive/subhi/saved_data/y_val.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(X_val.shape)"
      ],
      "metadata": {
        "id": "BsXKeS2M8xdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "audtj3sMV4LR"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, ReLU, Dropout, Flatten, Dense, BatchNormalization, TimeDistributed, LSTM\n",
        "from keras import metrics\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=4, input_shape=(129, 18, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "\n",
        "model.add(Conv2D(filters=51, kernel_size=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=4))\n",
        "\n",
        "model.add(Conv2D(filters=20, kernel_size=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Reshape((1, -1)))\n",
        "model.add(LSTM(58, return_sequences=False))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.build(input_shape=(129, 18, 3))\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[ metrics.MeanSquaredError(name='my_mse'),\n",
        "                                                                           metrics.AUC(name='my_auc'),])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvC8zkwOVmat"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, ReLU, Dropout, Flatten, Dense, BatchNormalization, TimeDistributed, LSTM\n",
        "from keras import metrics\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=4, input_shape=(129, 18, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(Conv2D(filters=32, kernel_size=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPooling2D(pool_size=4))\n",
        "model.add(Conv2D(filters=16, kernel_size=2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(ReLU())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Reshape((1, -1)))\n",
        "model.add(LSTM(58, return_sequences=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.build(input_shape=(129, 18, 3))\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[ metrics.MeanSquaredError(name='my_mse'),\n",
        "                                                                           metrics.AUC(name='my_auc'),])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sARCLJXl2cpD"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "gc.collect()\n",
        "\n",
        "#notes for improvement:\n",
        "# compare using bandpass filter (theta and alpha and beta)\n",
        "# use optuna on validation\n",
        "# EEGNET try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwcowCxLDEUl"
      },
      "outputs": [],
      "source": [
        "print(finX_train.shape)\n",
        "print(finX_test.shape)\n",
        "print(finX_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "X_train = []\n",
        "X_test = []\n",
        "X_val = []\n",
        "\n",
        "for i in range(21112):\n",
        "    img = Image.fromarray(finX_train[i].astype(np.uint8))\n",
        "    img_resized = img.resize((224, 224))\n",
        "    X_train.append(np.array(img_resized))\n",
        "\n",
        "for i in range(2484):\n",
        "    img = Image.fromarray(finX_test[i].astype(np.uint8))\n",
        "    img_resized = img.resize((224, 224))\n",
        "    X_test.append(np.array(img_resized))\n",
        "\n",
        "for i in range(1242):\n",
        "    img = Image.fromarray(finX_val[i].astype(np.uint8))\n",
        "    img_resized = img.resize((224, 224))\n",
        "    X_val.append(np.array(img_resized))"
      ],
      "metadata": {
        "id": "px_fnn3IHcxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(VGG16(weights='imagenet', input_shape=(224, 224, 3)))\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Qa7WNfinEwf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import metrics\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[ metrics.MeanSquaredError(name='my_mse'),\n",
        "                                                                           metrics.AUC(name='my_auc'),\n",
        "                                                                           'accuracy'])"
      ],
      "metadata": {
        "id": "4h11Rr_CF-D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "print(X_train.shape)\n",
        "X_val = np.array(X_val)"
      ],
      "metadata": {
        "id": "Njd2OgYmOJf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "bg9xskNFTa7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, 2)"
      ],
      "metadata": {
        "id": "4yEhGtr1UHLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-unet-collection"
      ],
      "metadata": {
        "id": "WYuQbYIZ0yxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LITHPmG05Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from keras_unet_collection import models\n",
        "\n",
        "# Define the input shape of your EEG spectrogram data\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Create the U-Net model\n",
        "unet_model = models.unet_2d(input_size=input_shape, n_labels=2, filter_num=[64, 128, 256, 512], batch_norm=True)\n",
        "\n",
        "# Add a GlobalAveragePooling2D layer to reduce the spatial dimensions to 1x1\n",
        "x = GlobalAveragePooling2D()(unet_model.output)\n",
        "\n",
        "# Add a Dense layer with 2 units for binary classification\n",
        "output = Dense(2, activation='sigmoid')(x)\n",
        "\n",
        "# Create a new model that includes the U-Net model and the Dense layer\n",
        "final_model = Model(inputs=unet_model.input, outputs=output)\n",
        "\n",
        "# Compile the final model\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "73mAsaY304rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.summary()"
      ],
      "metadata": {
        "id": "AUkWFXNR4khh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.fit(X_train, y_train, epochs=10, verbose=1, batch_size=16, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "bDja-BQvGt0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import selfeeg.models\n",
        "import torch"
      ],
      "metadata": {
        "id": "OvTjHEgzs5Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = selfeeg.models.DeepConvNet(2, 3, 21112)"
      ],
      "metadata": {
        "id": "obvALqsWtFrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(finX_train.shape)\n",
        "finX_train.reshape((2, 3, 21112))\n",
        "finX_train_tensor = torch.from_numpy(finX_train)\n",
        "print(finX_train_tensor.shape)\n",
        "\n",
        "output = model(finX_train_tensor)"
      ],
      "metadata": {
        "id": "ORHbgH0LxBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0ahzRLPSBeU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"upbet_model.h5\", monitor='val_', verbose=1, mode='auto', period=1)\n",
        "\n",
        "es=tensorflow.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=5,\n",
        "                                     verbose=1,  restore_best_weights=True)\n",
        "rlronp=tensorflow.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=3,\n",
        "                                             verbose=1)\n",
        "callbacks=[es, rlronp]\n",
        "history = model.fit(finX_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=20,\n",
        "            verbose=1,\n",
        "            validation_data=(finX_val, y_val),\n",
        "            callbacks=[callbacks, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s6CbTFt_t0l"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=15)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=50,\n",
        "            verbose=1,\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTLLomhvP8ZX"
      },
      "outputs": [],
      "source": [
        "model.save('best_auc.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN3UiDkCZxUu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['my_auc'])\n",
        "plt.plot(history.history['val_my_auc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2484):\n",
        "    img = Image.fromarray(finX_test[i].astype(np.uint8))\n",
        "    img_resized = img.resize((224, 224))\n",
        "    X_test.append(np.array(img_resized))"
      ],
      "metadata": {
        "id": "j_YPS-hFw_I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDqz5AWJxT9m",
        "outputId": "36c51ea1-4bfa-4ac5-b448-7551cfaf21f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2484, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtPpicLXDSjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# best_mode = load_model('/content/drive/MyDrive/subhi/saved_data/sthuper_best_model.h5')\n",
        "y_pred_prob = final_model.predict(X_test)\n",
        "\n",
        "y_test_absolute = y_test[:, 1]\n",
        "print(y_test_absolute)\n",
        "y_pred_abs = np.where(y_pred_prob[:, 1] > 0.5, 1, 0)\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test_absolute, y_pred_abs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_WXt-iNU96x"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_absolute, y_pred_abs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['True Negative', 'True Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLXASEvk50nw"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "model.fit(X_train, dataset1_y, epochs=10,\n",
        "          batch_size=32, verbose=1, callbacks= checkpoint)\n",
        "\n",
        "\n",
        "#Reload model\n",
        "model = load_model('best_model.hdf5')\n",
        "\n",
        "#Continue training\n",
        "dataset2_x = X_train[3000:]\n",
        "dataset2_y = y_train[3000:]\n",
        "model.fit(dataset2_x, dataset2_y, epoch=10, batch_size=200, verbose=2)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAGR_gg30Pz-"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/subhi/saved_data/best_model.hdf5\")\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=16,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGlhAr1k21je"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "test = []\n",
        "real = []\n",
        "RND_NUMS = [random.randint(0, 17130) for _ in range(20)]\n",
        "for r in RND_NUMS:\n",
        "    test.append(X_train[r])\n",
        "    real.append(y_train[r])\n",
        "new_model = keras.models.load_model(\"/content/drive/MyDrive/subhi/saved_data/best_model.hdf5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLN9GMMGy8uL"
      },
      "outputs": [],
      "source": [
        "predictions = new_model.predict(np.array(test))\n",
        "pred = np.argmax(predictions, axis=1)\n",
        "print(pred)\n",
        "print(real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbzGtGN-WF1j"
      },
      "outputs": [],
      "source": [
        "def data_generator(data, labels, batch_size):\n",
        "    num_samples = len(data)\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        batch_data = data[batch_indices]\n",
        "        batch_labels = labels[batch_indices]\n",
        "        yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYZ4vc5hWS0k"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "batch_size = 100\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    for batch_data, batch_labels in data_generator(X_train, y, batch_size):\n",
        "        model.train_on_batch(batch_data, batch_labels)\n",
        "    del batch_data, batch_labels\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1K06XznNqnw3InowfjGNIIfLLJS5mbv2S",
      "authorship_tag": "ABX9TyO8EkN0LCugvqSFi2+h/AcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}